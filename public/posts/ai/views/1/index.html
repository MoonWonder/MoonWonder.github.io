<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>Ai随想1——Linear Attention真的能成功吗？ - MoonWonder</title><meta name=author content="MoonWonder"><meta name=description content="信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？ 大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。
"><meta name=keywords content='AI,Linear Attention'><meta itemprop=name content="ai随想1——Linear Attention真的能成功吗？"><meta itemprop=description content="信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？ 大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。"><meta itemprop=datePublished content="2025-05-25T11:06:08+08:00"><meta itemprop=dateModified content="2025-10-28T09:28:15+08:00"><meta itemprop=wordCount content="2107"><meta itemprop=image content="https://moonwonder.top/logo.png"><meta itemprop=keywords content="AI,Linear Attention"><meta property="og:url" content="https://moonwonder.top/posts/ai/views/1/"><meta property="og:site_name" content="MoonWonder"><meta property="og:title" content="ai随想1——Linear Attention真的能成功吗？"><meta property="og:description" content="信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？ 大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-25T11:06:08+08:00"><meta property="article:modified_time" content="2025-10-28T09:28:15+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="Linear Attention"><meta property="og:image" content="https://moonwonder.top/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://moonwonder.top/logo.png"><meta name=twitter:title content="ai随想1——Linear Attention真的能成功吗？"><meta name=twitter:description content="信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？ 大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。"><meta name=application-name content="MoonWonder"><meta name=apple-mobile-web-app-title content="MoonWonder"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical type=text/html href=https://moonwonder.top/posts/ai/views/1/ title="ai随想1——Linear Attention真的能成功吗？ - MoonWonder"><link rel=prev type=text/html href=https://moonwonder.top/posts/chemistry/review1/ title=化学复习笔记1><link rel=next type=text/html href=https://moonwonder.top/posts/ai/frame/verl1/ title=AI架构学习笔记><link rel=alternate type=text/plain href=https://moonwonder.top/posts/ai/views/1/index.md title="ai随想1——Linear Attention真的能成功吗？ - MoonWonder"><link rel=stylesheet href=/css/config.min.251880c471e90abe5f3e03b6b6ff7ebf.css integrity="md5-JRiAxHHpCr5fPgO2tv9+vw=="><link rel=stylesheet href=/css/style.min.c3fbd99ca3a9e2cb28f2cffa1a3a4541.css integrity="md5-w/vZnKOp4sso8s/6GjpFQQ=="><link rel=preload href=/lib/fontawesome-free/all.min.bde2a1ac06ec87a30967e54da50f5c15.css integrity="md5-veKhrAbsh6MJZ+VNpQ9cFQ==" as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.bde2a1ac06ec87a30967e54da50f5c15.css integrity="md5-veKhrAbsh6MJZ+VNpQ9cFQ=="></noscript><link rel=preload href=/lib/animate/animate.min.c0be8e53226ac34833fd9b5dbc01ebc5.css integrity="md5-wL6OUyJqw0gz/ZtdvAHrxQ==" as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.c0be8e53226ac34833fd9b5dbc01ebc5.css integrity="md5-wL6OUyJqw0gz/ZtdvAHrxQ=="></noscript><meta name=google-site-verification content="MQ8DNu27ayX6B_4ObiEDK09vGr1fdy7kOAnbd09hJk4"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"ai随想1——Linear Attention真的能成功吗？","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/moonwonder.top\/posts\/ai\/views\/1\/"},"image":["https:\/\/moonwonder.top\/images\/Apple-Devices-Preview.png"],"genre":"posts","keywords":"AI, Linear Attention","wordcount":2107,"url":"https:\/\/moonwonder.top\/posts\/ai\/views\/1\/","datePublished":"2025-05-25T11:06:08+08:00","dateModified":"2025-10-28T09:28:15+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/moonwonder.top\/images\/avatar.png"},"author":{"@type":"Person","name":"MoonWonder"},"description":""}</script><script src=/js/head/color-scheme.min.9577b206ee497277eba5813880d7978c.js integrity="md5-lXeyBu5JcnfrpYE4gNeXjA=="></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=MoonWonder><span class=header-title-pre><svg class="icon" viewBox="0 0 576 512"><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1.0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7.0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174 402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7-43.2-43.2c-4.1-4.1-10.8-4.1-14.8.0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg></span><span class=typeit><template>MoonWonder</template></span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class=menu-item><a class=menu-link href=/books/>系列</a></li><li class=menu-item><a class=menu-link href=/about/>关于</a></li><li class=menu-item><a class=menu-link href=/heatmap/>热力图</a></li><li class=menu-item><a class=menu-link href=/link/>友链</a></li><li class=menu-item><a class=menu-link href=https://github.com/MoonWonder/moonwonder.github.io title=GitHub rel="noopener noreferrer" target=_blank><svg class="icon" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li><li class="menu-item language-switch"><span role=button aria-label=选择语言 title=选择语言><i class="fa-solid fa-language fa-fw" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item>没有更多翻译</li></ul></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=MoonWonder><span class=header-title-pre><svg class="icon" viewBox="0 0 576 512"><path d="M402.3 344.9l32-32c5-5 13.7-1.5 13.7 5.7V464c0 26.5-21.5 48-48 48H48c-26.5.0-48-21.5-48-48V112c0-26.5 21.5-48 48-48h273.5c7.1.0 10.7 8.6 5.7 13.7l-32 32c-1.5 1.5-3.5 2.3-5.7 2.3H48v352h352V350.5c0-2.1.8-4.1 2.3-5.6zm156.6-201.8L296.3 405.7l-90.4 10c-26.2 2.9-48.5-19.2-45.6-45.6l10-90.4L432.9 17.1c22.9-22.9 59.9-22.9 82.7.0l43.2 43.2c22.9 22.9 22.9 60 .1 82.8zM460.1 174 402 115.9 216.2 301.8l-7.3 65.3 65.3-7.3L460.1 174zm64.8-79.7-43.2-43.2c-4.1-4.1-10.8-4.1-14.8.0L436 82l58.1 58.1 30.9-30.9c4-4.2 4-10.8-.1-14.9z"/></svg></span><span class=typeit><template>MoonWonder</template></span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/>所有文章</a></li><li class=menu-item><a class=menu-link href=/tags/>标签</a></li><li class=menu-item><a class=menu-link href=/categories/>分类</a></li><li class=menu-item><a class=menu-link href=/books/>系列</a></li><li class=menu-item><a class=menu-link href=/about/>关于</a></li><li class=menu-item><a class=menu-link href=/heatmap/>热力图</a></li><li class=menu-item><a class=menu-link href=/link/>友链</a></li><li class=menu-item><a class=menu-link href=https://github.com/MoonWonder/moonwonder.github.io title=GitHub rel="noopener noreferrer" target=_blank><svg class="icon" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span><span class="menu-system-item language-switch">
<span role=button aria-label=选择语言 title=选择语言>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span>
<select class=language-select onchange="location=this.value"><option disabled>没有更多翻译</option></select></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=fi-container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>Ai随想1——Linear Attention真的能成功吗？</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/moonwonder title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><img class=avatar src=/images/avatar.png alt=MoonWonder height=16 width=16>&nbsp;MoonWonder</a></span></div><div class=post-meta-line><span title="发布于 2025-05-25 11:06:08"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2025-05-25>2025-05-25</time></span>&nbsp;<span title="更新于 2025-10-28 09:28:15"><i class="fa-regular fa-calendar-check fa-fw me-1" aria-hidden=true></i><time datetime=2025-10-28>2025-10-28</time></span>&nbsp;<span title="2107 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 2200 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 5 分钟</span>&nbsp;<span id=/posts/ai/views/1/ class="leancloud_visitors comment-visitors" data-flag-title="Ai随想1——Linear Attention真的能成功吗？"><i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span class=leancloud-visitors-count>-</span>&nbsp;次阅读
</span>&nbsp;<span class=comment-count data-flag-title="Ai随想1——Linear Attention真的能成功吗？">
<i class="fa-regular fa-comments fa-fw me-1" aria-hidden=true></i><span data-xid=/posts/ai/views/1/ class=valine-comment-count>-</span>&nbsp;条评论
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#信息瓶颈像-rwkv-这样的线性注意力模型是否压缩过紧>信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？</a><ul><li><a href=#理想-vs-现实信息需求>理想 vs. 现实：信息需求</a></li><li><a href=#rwkv-6-与一个计算得出的-25-利用率>RWKV-6 与一个计算得出的 25% 利用率？</a></li><li><a href=#对无损上下文的追求与--的阴影>对“无损”上下文的追求与 的阴影</a></li><li><a href=#结论一场持续的平衡之舞>结论：一场持续的平衡之舞</a></li></ul></li></ul></nav></div></div><div class=content id=content><h2 class=heading-element id=信息瓶颈像-rwkv-这样的线性注意力模型是否压缩过紧><span>信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？</span>
<a href=#%e4%bf%a1%e6%81%af%e7%93%b6%e9%a2%88%e5%83%8f-rwkv-%e8%bf%99%e6%a0%b7%e7%9a%84%e7%ba%bf%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%9e%8b%e6%98%af%e5%90%a6%e5%8e%8b%e7%bc%a9%e8%bf%87%e7%b4%a7 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。</p><p>近期的计算和领域内的观察揭示了一种有趣的权衡。让我们深入探讨其中一些见解，特别是关于采用线性注意力机制的模型，如 RWKV。</p><hr><h3 class=heading-element id=理想-vs-现实信息需求><span>理想 vs. 现实：信息需求</span>
<a href=#%e7%90%86%e6%83%b3-vs-%e7%8e%b0%e5%ae%9e%e4%bf%a1%e6%81%af%e9%9c%80%e6%b1%82 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>理论上，要有效捕捉长度为 <strong>&rsquo;n&rsquo;</strong> 的序列中所有细微的依赖关系，像 <strong>MQAR (多查询注意力与旋转位置编码)</strong> 这样的注意力机制可能需要处理或表示至少 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \log n</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal">n</span><span class=mspace style=margin-right:.1667em></span><span class=mop>lo<span style=margin-right:.01389em>g</span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal">n</span></span></span></span> 比特量级的信息。这是一个理论基准，代表了理想注意力机制所能处理的连接和信息的丰富程度。可以将其想象为完全描绘出长文本中所有重要词语间关系所需的数据量。</p><p>现在，让我们考虑<strong>线性注意力 (Linear Attention)</strong> 模型。这些架构是一个突破，因为它们将注意力机制的计算复杂度从随序列长度的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>（平方级）降低到了 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mclose>)</span></span></span></span>（线性级）。这对于处理更长序列而无需担心 GPU 过载来说是一个巨大的胜利。RWKV 是成功实现这种线性方法的一个著名例子，使其能够在推理时作为循环神经网络 (RNN) 运行，从而提供了效率。</p><p>然而，这种效率可能需要以信息吞吐量为代价。据估计，线性注意力机制，特别是考虑到其内部状态表示时，在其每一步能够“携带”或处理的信息量方面可能受到限制。如果我们考虑这类模型的隐藏状态维度 <strong>&rsquo;d&rsquo;</strong>，其信息容量可能更多地与 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d^2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 相关。</p><hr><h3 class=heading-element id=rwkv-6-与一个计算得出的-25-利用率><span>RWKV-6 与一个计算得出的 25% 利用率？</span>
<a href=#rwkv-6-%e4%b8%8e%e4%b8%80%e4%b8%aa%e8%ae%a1%e7%ae%97%e5%be%97%e5%87%ba%e7%9a%84-25-%e5%88%a9%e7%94%a8%e7%8e%87 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>根据用户的计算，基于 <strong>FP8 精度</strong>（每个数字使用 8 比特存储），一个线性注意力模型可以有效地传递或利用一个包含大约 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>⋅</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">d^2 \cdot 8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span></span></span></span> 比特信息的状态。</p><p>比较这两个数字——全面注意力理论上需要的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \log n</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal">n</span><span class=mspace style=margin-right:.1667em></span><span class=mop>lo<span style=margin-right:.01389em>g</span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal">n</span></span></span></span> 比特和线性注意力状态的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>⋅</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">d^2 \cdot 8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span></span></span></span> 比特容量——得出了一个有趣（且发人深省）的计算结果：</p><p>如果 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>⋅</mo><mn>8</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo>⁡</mo><mi>n</mi><mo stretchy="false">)</mo><mo>≈</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">(d^2 \cdot 8) / (n \log n) \approx 0.25</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class=mord>8</span><span class=mclose>)</span><span class=mord>/</span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mspace style=margin-right:.1667em></span><span class=mop>lo<span style=margin-right:.01389em>g</span></span><span class=mspace style=margin-right:.1667em></span><span class="mord mathnormal">n</span><span class=mclose>)</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>≈</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>0.25</span></span></span></span>，这将意味着对于给定的序列长度 &rsquo;n&rsquo; 和隐藏维度 &rsquo;d&rsquo;，像 <strong>RWKV-6</strong> 这样的模型可能<strong>仅利用了更具表现力（但计算成本更高）的注意力机制所期望达到的理论信息容量的约 25%</strong>。</p><p>这并不意味着 RWKV-6 无效；其在现实世界中的表现已经证明了自己。相反，这突显出当前高效线性注意力机制的信息处理能力与理论上无需任何损失即可捕获<em>所有</em>潜在长程依赖关系所需的能力之间可能存在显著差距。该模型很可能学会了在其确实拥有的比特数下实现高度效率，专注于最重要的信息。</p><hr><h3 class=heading-element id=对无损上下文的追求与--的阴影><span>对“无损”上下文的追求与 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span> 的阴影</span>
<a href=#%e5%af%b9%e6%97%a0%e6%8d%9f%e4%b8%8a%e4%b8%8b%e6%96%87%e7%9a%84%e8%bf%bd%e6%b1%82%e4%b8%8e--%e7%9a%84%e9%98%b4%e5%bd%b1 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>“现在的linear-attention其实只能做到n*(sqrt(n)^2)也就是n^2的复杂度下才能无损上下文。”</p></blockquote><p>让我们解读一下。线性注意力的主要吸引力在于其每一步的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class="mord mathnormal">n</span><span class=mclose>)</span></span></span></span> 计算扩展性。但是，如果我们要求“无损”上下文呢？这里的“无损”可能意味着保留与二次注意力机制（具有 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span> 复杂度）理论上可以捕获的相同水平的细节和依赖信息。</p><p>该论点表明，对于当前的线性注意力机制，要在长度为 &rsquo;n&rsquo; 的序列上真正实现这种“无损”状态，所需的有效资源或内部表征能力可能隐含地以一种反映 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span> 的方式扩展。例如，如果隐藏维度 &rsquo;d&rsquo; 需要与 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mi>n</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{n}</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.04em;vertical-align:-.2397em></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r><span class=vlist style=height:.8003em><span class=svg-align style=top:-3em><span class=pstrut style=height:3em></span><span class=mord style=padding-left:.833em><span class="mord mathnormal">n</span></span></span><span style=top:-2.7603em><span class=pstrut style=height:3em></span><span class=hide-tail style=min-width:.853em;height:1.08em><svg width="400em" height="1.08em" viewBox="0 0 4e5 1080" preserveAspectRatio="xMinYMin slice"><path d="M95 702c-2.7.0-7.17-2.7-13.5-8-5.8-5.3-9.5-10-9.5-14 0-2 .3-3.3 1-4 1.3-2.7 23.83-20.7 67.5-54 44.2-33.3 65.8-50.3 66.5-51 1.3-1.3 3-2 5-2 4.7.0 8.7 3.3 12 10s173 378 173 378c.7.0 35.3-71 104-213s137.5-285 206.5-429S812 97.3 814 94c5.3-9.3 12-14 20-14H4e5v40H845.2724s-225.272 467-225.272 467-235 486-235 486c-2.7 4.7-9 7-19 7-6 0-10-1-12-3s-194-422-194-422-65 47-65 47zM834 80h4e5v40H834z"/></svg></span></span></span><span class=vlist-s>​</span></span><span class=vlist-r><span class=vlist style=height:.2397em><span></span></span></span></span></span></span></span></span> 成比例增长以在 &rsquo;n&rsquo; 增加时保持信息保真度，那么我们容量估计（<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>⋅</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">d^2 \cdot 8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span></span></span></span> 比特）中的 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d^2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 项将变得与 &rsquo;n&rsquo; 成比例。</p><p>这并不一定意味着线性注意力的<em>计算步骤</em>会再次变为 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:1.0641em;vertical-align:-.25em></span><span class="mord mathnormal" style=margin-right:.02778em>O</span><span class=mopen>(</span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose>)</span></span></span></span>。相反，它可能意味着实现“无损”所需的<em>总信息容量或表征丰富度</em>在根本上仍然以接近 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>（序列中所有可能的成对交互的数量）的规模扩展。如果线性注意力每步的容量没有固有地如此快速增长，那么在没有其他架构创新或显著更大的 &rsquo;d&rsquo; 值（这反过来又会增加计算和内存需求）的情况下，它可能难以在非常长的序列上实现真正的“无损”。</p><p>同样至关重要的是要记住，这些大O表示法中的<strong>常数因子</strong>在实际性能和容量中可能扮演重要角色。<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>⋅</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">d^2 \cdot 8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">d</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mspace style=margin-right:.2222em></span><span class=mbin>⋅</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span></span></span></span> 中的“8”就是这样一个与 FP8 相关的因子。其他架构细节可能会引入不同的常数。</p><hr><h3 class=heading-element id=结论一场持续的平衡之舞><span>结论：一场持续的平衡之舞</span>
<a href=#%e7%bb%93%e8%ae%ba%e4%b8%80%e5%9c%ba%e6%8c%81%e7%bb%ad%e7%9a%84%e5%b9%b3%e8%a1%a1%e4%b9%8b%e8%88%9e class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>这些观察结果强调了 LLM 架构设计中复杂的权衡：</p><ul><li><strong>标准（二次）注意力</strong>为捕获依赖关系提供了丰富的容量，但在处理较长序列时会遇到计算瓶颈。</li><li>**线性注意力（如 RWKV 中的）**突破了计算瓶颈，实现了更长的上下文窗口，但可能面临信息瓶颈，与理论上的理想情况甚至二次注意力所能保留的信息（尽管成本更高）相比，可能会“有损”。</li></ul><p>如果特定的模型配置和序列长度证实了 RWKV-6 的 25% 利用率这一数字，那么这将是对这种潜在瓶颈的一个鲜明提醒。认为当前线性注意力的“无损”上下文可能仍带有 <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n^2</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.8141em></span><span class=mord><span class="mord mathnormal">n</span><span class=msupsub><span class=vlist-t><span class=vlist-r><span class=vlist style=height:.8141em><span style=top:-3.063em;margin-right:.05em><span class=pstrut style=height:2.7em></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 复杂度的阴影（就所需信息而言，如果不是直接的每步计算）的观点表明，我们尚未完全摆脱长序列带来的挑战。</p><p>研究仍在不断突破界限，探索新的注意力机制、不同的信息压缩和检索方法以及混合方法。目标始终如一：构建能够高效理解海量上下文且不丢失关键信息的模型。显然，实现这一目标的旅程仍然充满着激动人心的挑战和巧妙的解决方案。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2025-10-28 09:28:15">更新于 2025-10-28&nbsp;</span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/ai/views/1/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 X" data-sharer=twitter data-url=https://moonwonder.top/posts/ai/views/1/ data-title="Ai随想1——Linear Attention真的能成功吗？" data-hashtags="AI,Linear Attention"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://moonwonder.top/posts/ai/views/1/ data-hashtag=AI><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Hacker News" data-sharer=hackernews data-url=https://moonwonder.top/posts/ai/views/1/ data-title="Ai随想1——Linear Attention真的能成功吗？"><i class="fa-brands fa-hacker-news fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://moonwonder.top/posts/ai/views/1/ data-title="Ai随想1——Linear Attention真的能成功吗？"><svg class="icon" role="img" viewBox="0 0 24 24"><title>LINE</title><path d="M19.365 9.863c.349.0.63.285.63.631.0.345-.281.63-.63.63H17.61v1.125h1.755c.349.0.63.283.63.63.0.344-.281.629-.63.629h-2.386c-.345.0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63h2.386c.346.0.627.285.627.63.0.349-.281.63-.63.63H17.61v1.125h1.755zm-3.855 3.016c0 .27-.174.51-.432.596-.064.021-.133.031-.199.031-.211.0-.391-.09-.51-.25l-2.443-3.317v2.94c0 .344-.279.629-.631.629-.346.0-.626-.285-.626-.629V8.108c0-.27.173-.51.43-.595.06-.023.136-.033.194-.033.195.0.375.104.495.254l2.462 3.33V8.108c0-.345.282-.63.63-.63.345.0.63.285.63.63v4.771zm-5.741.0c0 .344-.282.629-.631.629-.345.0-.627-.285-.627-.629V8.108c0-.345.282-.63.63-.63.346.0.628.285.628.63v4.771zm-2.466.629H4.917c-.345.0-.63-.285-.63-.629V8.108c0-.345.285-.63.63-.63.348.0.63.285.63.63v4.141h1.756c.348.0.629.283.629.63.0.344-.282.629-.629.629M24 10.314C24 4.943 18.615.572 12 .572S0 4.943.0 10.314c0 4.811 4.27 8.842 10.035 9.608.391.082.923.258 1.058.59.12.301.079.766.038 1.08l-.164 1.02c-.045.301-.24 1.186 1.049.645 1.291-.539 6.916-4.078 9.436-6.975C23.176 14.393 24 12.458 24 10.314"/></svg></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://moonwonder.top/posts/ai/views/1/ data-title="Ai随想1——Linear Attention真的能成功吗？" data-ralateuid=7562571916><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/ai/ class=post-tag title="标签 - AI">AI</a><a href=/tags/linear-attention/ class=post-tag title="标签 - Linear Attention">Linear Attention</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/chemistry/review1/ class=post-nav-item rel=prev title=化学复习笔记1><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>化学复习笔记1</a><a href=/posts/ai/frame/verl1/ class=post-nav-item rel=next title=AI架构学习笔记>AI架构学习笔记<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/ rel="external nofollow noopener noreferrer">Valine</a>.</noscript></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.147.9"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.4.0-alpha.2-20251023040336-063af2cd"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2021 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=https://github.com/moonwonder target=_blank rel="external nofollow noopener noreferrer">MoonWonder</a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreferrer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div><div class="fixed-button view-comments d-none" role=button aria-label=查看评论><i class="fa-solid fa-comment fa-fw" aria-hidden=true></i></div></div><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=/lib/lightgallery/css/lightgallery-bundle.min.7e80d25d294e8842d39022810b958552.css integrity="md5-foDSXSlOiELTkCKBC5WFUg=="><link rel=preload href=/lib/katex/katex.min.b6536013af151fbdc29bbdf0f10e1f77.css integrity="md5-tlNgE68VH73Cm73w8Q4fdw==" as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/katex/katex.min.b6536013af151fbdc29bbdf0f10e1f77.css integrity="md5-tlNgE68VH73Cm73w8Q4fdw=="></noscript><script src=/lib/valine/Valine.min.91ad455f2b872ce889395474e4609481.js integrity="md5-ka1FXyuHLOiJOVR05GCUgQ=="></script><script src=/lib/autocomplete/autocomplete.min.553edd1b8dd335ab4626f05e6131812a.js integrity="md5-VT7dG43TNatGJvBeYTGBKg==" defer></script><script src=/lib/algoliasearch/algoliasearch-lite.umd.min.339e44173f7561e5ba328301e972bc2f.js integrity="md5-M55EFz91YeW6MoMB6XK8Lw==" defer></script><script src=/lib/instant-page/instantpage.min.d4be8390e577e842090e98303563633e.js integrity="md5-1L6DkOV36EIJDpgwNWNjPg==" async defer type=module></script><script src=/lib/lightgallery/lightgallery.min.b83f93b7f9e08420c662a3229d70357e.js integrity="md5-uD+Tt/nghCDGYqMinXA1fg==" defer></script><script src=/lib/lightgallery/plugins/thumbnail/lg-thumbnail.min.bd0822f7901ccdaa2ee841c69101449e.js integrity="md5-vQgi95Aczaou6EHGkQFEng==" defer></script><script src=/lib/lightgallery/plugins/zoom/lg-zoom.min.a775775d139d5670bc75288e68b6aedb.js integrity="md5-p3V3XROdVnC8dSiOaLau2w==" defer></script><script src=/lib/sharer/sharer.min.9c17fd8602aab18b8337139614a30fda.js integrity="md5-nBf9hgKqsYuDNxOWFKMP2g==" async defer></script><script src=/lib/typeit/index.umd.10b1c79fdaa004fa101077c3fc636a5a.js integrity="md5-ELHHn9qgBPoQEHfD/GNqWg==" defer></script><script src=/lib/katex/copy-tex.min.6be74da2bd31a1975da5090d5926cd54.js integrity="md5-a+dNor0xoZddpQkNWSbNVA==" defer></script><script src=/posts/ai/views/1/config/page.js defer></script><script src=/js/theme.min.e245c9c9d0171e1e3ecf45805f56a0f8.js integrity="md5-4kXJydAXHh4+z0WAX1ag+A==" defer></script></body></html>