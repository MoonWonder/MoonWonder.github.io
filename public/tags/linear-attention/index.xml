<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linear Attention - 标签 - MoonWonder</title><link>https://moonwonder.top/tags/linear-attention/</link><description>MoonWonder的博客</description><generator>Hugo 0.147.9 &amp; FixIt v0.4.0-alpha.2-20251023040336-063af2cd</generator><language>zh-CN</language><managingEditor>ybw051114@qq.com (MoonWonder)</managingEditor><webMaster>ybw051114@qq.com (MoonWonder)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 28 Oct 2025 09:28:15 +0800</lastBuildDate><atom:link href="https://moonwonder.top/tags/linear-attention/index.xml" rel="self" type="application/rss+xml"/><item><title>ai随想1——Linear Attention真的能成功吗？</title><link>https://moonwonder.top/posts/ai/views/1/</link><pubDate>Sun, 25 May 2025 11:06:08 +0800</pubDate><author>ybw051114@qq.com (MoonWonder)</author><guid>https://moonwonder.top/posts/ai/views/1/</guid><description>&lt;h2 class="heading-element" id="信息瓶颈像-rwkv-这样的线性注意力模型是否压缩过紧">&lt;span>信息瓶颈：像 RWKV 这样的线性注意力模型是否“压缩”过紧？&lt;/span>
 &lt;a href="#%e4%bf%a1%e6%81%af%e7%93%b6%e9%a2%88%e5%83%8f-rwkv-%e8%bf%99%e6%a0%b7%e7%9a%84%e7%ba%bf%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%a8%a1%e5%9e%8b%e6%98%af%e5%90%a6%e5%8e%8b%e7%bc%a9%e8%bf%87%e7%b4%a7" class="heading-mark">
 &lt;svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">&lt;path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z">&lt;/path>&lt;/svg>
 &lt;/a>
&lt;/h2>&lt;p>大语言模型 (LLM) 正在彻底改变我们与信息交互的方式，但它们处理和“记忆”长文本的能力——即上下文窗口——是一个关键的角力场。我们都希望模型能理解整本书，而不仅仅是段落。然而，扩展这个上下文窗口并非易事，常常会遇到计算瓶颈，或者更微妙地，信息瓶颈。&lt;/p></description></item></channel></rss>