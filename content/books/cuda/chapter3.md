---
title: 核函数编写与优化技巧
subtitle:
date: 2025-11-19T23:21:06+08:00
description:
keywords:
draft: false
---

> 掌握了 CUDA 编程的基本概念和开发环境后，我们开始编写自己的 CUDA 核函数（Kernel），并学习如何优化 GPU 上的代码。本章将讨论核函数的编写规范，包括关键字和内存操作，以及几项 GPU 编程中特有的优化技巧：**避免 Warp 分支发散**、**使用共享内存优化访存**、**确保全局内存访问对齐和合并**等。通过这些实践，读者将学习编写高效 CUDA 内核的基本方法。

### 3.1 编写 CUDA 核函数

**核函数定义：** CUDA 扩展 C/C++ 语法，使用 `__global__` 关键字来声明一个函数为 GPU 核函数。例如：

```cpp
__global__ void saxpy(int n, float a, float *x, float *y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}
```

上例是经典的 SAXPY (y = a*x + y) 操作的 GPU 实现。我们通过 `__global__` 指示这是在设备上执行的函数。该函数可以被主机代码调用，通过 `saxpy<<<gridDim, blockDim>>>(...)` 的语法启动。

**核函数调用：** 核函数调用中 `<<<...>>>` 称为**执行配置**或 launch configuration，里面通常两个参数：第一个是 grid 大小（线程块数），第二个是 block 大小（每块线程数）。可选第三个参数用于动态分配共享内存大小，第四个参数指定使用哪个 CUDA stream（默认为 0，即默认流）。

例如：`saxpy<<<(n+255)/256, 256>>>(N, 2.0f, d_x, d_y);` 启动一个 grid，其块数为 `(n+255)/256`（确保覆盖 n 个元素），每块 256 线程。

**内存管理 API：** 在主机代码中，我们通过 CUDA runtime API 管理 GPU 内存，如：

*   `cudaMalloc(&dev_ptr, bytes)` 在设备上分配内存。
    
*   `cudaMemcpy(dst, src, bytes, cudaMemcpyHostToDevice/DeviceToHost)` 拷贝数据。
    
*   `cudaFree(dev_ptr)` 释放设备内存。  
    这些 API 返回 `cudaError_t` 错误码，最好每次调用后检查错误（后面 3.4 节详述错误检查实践）。
    

**线程同步：** 在核函数内部，可使用`__syncthreads()`使一个线程块内所有线程同步，即等待所有线程执行到此处再继续后面的执行。这在需要确保共享内存数据已被所有线程加载完毕时非常重要。在块内共享内存协作的代码里，经常会看到类似结构：

```cpp
// 每个线程将部分全局数据加载到共享内存
shared_mem[threadIdx.x] = global_data[index];
__syncthreads();  // 确保共享内存填充完毕
// 后续线程使用共享内存数据进行计算
result = shared_mem[some_index] * 5;
```

对于块内同步，`__syncthreads()` 是必须的。如果缺少它，可能出现有的线程还未写入共享内存，其他线程就开始读取旧数据，导致结果错误。

注意，`__syncthreads()` **只能同步同一块内的线程**，不同线程块之间无法用此方法同步（因为它们不在同一 SM，可彼此独立执行）。不同块间的同步通常需要将结果写回全局内存，然后在主机端用 `cudaDeviceSynchronize()` 等等待所有 kernel 完成，或拆分为多个 kernel 顺序调用来实现。

**常见 CUDA 关键字:**

*   `__global__` 表示核函数，由 CPU 调用，在 GPU 上执行。
    
*   `__device__` 表示设备函数，只能从 GPU 上的核函数或其他设备函数调用（即只能由 GPU 代码调用，在 GPU 上执行）。
    
*   `__host__` 可以标记在主机上执行的函数，通常默认主机函数不需要特别标记，除非一个函数同时定义了 `__host__` 和 `__device__`（使其可在两端使用）。
    
*   `__shared__` 用于在核函数内定义共享内存变量，例如：`__shared__ float tile[16][16];`，块内线程共享这块存储。
    
*   `__constant__` 可定义常量内存上的变量。
    
*   内建变量如 `threadIdx, blockIdx, blockDim, gridDim` 用于获取线程 / 块的索引和尺寸信息。
    

**指数、数学函数：** CUDA 提供大量 C 标准数学函数的设备版本，比如 `sinf, cosf, expf` 等，也有 `__syncthreads()` 以外的一些并行原语如 warp shuffle 等（高级主题，此处不展开）。

总之，编写核函数与 C 函数类似，但要考虑并行执行的特点。初学者编写核函数时，建议先从简单循环的等价并行版本开始，比如**将 for 循环迭代并行化**：让每个线程处理原先循环中的一次迭代。例如，原先 CPU 代码：

```cpp
for(int i=0; i<n; ++i) C[i] = A[i] + B[i];
```

可以转为 GPU 核函数：

```cpp
int i = blockIdx.x * blockDim.x + threadIdx.x;
if(i < n) C[i] = A[i] + B[i];
```

并让 `(n+threadsPerBlock-1)/threadsPerBlock` 个块的每块 `threadsPerBlock` 个线程并行执行，即实现了将循环拆分到多个线程上。

### 3.2 避免 Warp 分支发散的技巧

上一章讨论了 warp 分支发散的危害，这里我们总结一些减少发散的实用技巧：

*   **使条件与线程 ID 关联均匀**：如果可以，避免让同一 warp 内只有少数线程满足条件、其他多数不满足的情况。理想情况是 warp 内线程要么全进 if 分支，要么全进 else 分支。例如，当需要根据线程索引做条件判断时，可考虑每个 warp 统一的条件阈值。举例：不要用 `if(threadIdx.x == 0) ...`（这将每个 warp 仅一个线程执行 if），而可以将需要单线程执行的任务移到块级（如利用 blockIdx 判断）。
    
*   **使用掩码或条件赋值替代显式分支：** 在一些简单情形下，可以利用三目运算符或逻辑表达式消除 if。例如：
    
    ```cpp
    // 有分支的情况
    if(x > 0) y = x; else y = 0;
    // 改写为
    y = (x > 0 ? x : 0);
    ```
    
    GPU 编译器可能将其转换为谓词指令，使 warp 内不同条件的线程不发生分支而采用 “计算再选择” 的方式。像之前提到的通过**显式计算两种结果再选择**就是避免分支的策略。当然这可能增加额外计算，但如果分支发散开销更大时，这样做是值得的。
    
*   **warp 投票函数与投影：** CUDA 提供一些 warp 级函数如 `__all_sync`，`__any_sync`，`__ballot_sync` 等，可用于 warp 中线程协作判断条件。这有助于 warp 统一决策，从而统一执行路径。例如若 warp 内需要依据某条件执行某段代码，使用 `__any_sync` 检查 warp 内是否有任一线程满足，可由一个 warp 内代表线程执行，其他线程通过条件避免实际执行重复逻辑。这些进阶用法需结合 shuffle 或较复杂逻辑，不是初学内容，但知道有这样的工具很重要。
    
*   **尽量结构化程序：** GPU 编译器对**循环展开**、**分支预测**等也有一定优化能力。如果判断逻辑复杂，考虑是否可以拆分为多个 kernel，在不同 kernel 中分别处理各类情况，从而在每个 kernel 内部减少分支。
    

最后要说明，有时分支是算法固有的，不可能完全避免。这种情况下，我们至少可以**减少发散程度**：例如调整数据排列，让 warp 内线程更可能执行相同分支（数据重组技术）；或者在性能要求不高的代码里忽略发散问题，保证代码清晰正确。这需要权衡。但对于性能关键的核函数，务必仔细分析 warp 执行情况。借助 Nsight 分析工具，可以测量每个核函数的 “分支发散率” 等指标来定位问题。

### 3.3 利用共享内存优化访存

**共享内存**是提升 CUDA 性能的利器之一。由于共享内存速度远快于全局显存（延迟大约几百个时钟 vs. 上千个时钟），充分利用共享内存可以显著降低访存瓶颈。典型的用法是 ** 数据分块（tiling）** 技术：将需要反复使用的数据片段读取到共享内存，令同一块内多个线程重复使用，减少重复的全局内存访问。

例如，在矩阵乘法中计算 C = A * B，我们知道点积计算时矩阵 A 的一行元素会与矩阵 B 的一列元素各自重复使用 N 次。如果每次乘法都从全局内存取数，会有很多冗余访存。而使用 tiling，每个线程块负责计算 C 的一个子矩阵（如 16×16），先将对应的 A 子块和 B 子块加载到共享内存，然后让 block 内线程各自计算部分结果。由于共享内存中的数据线程可以多次访问，这就大大降低了全局内存带宽需求。

使用共享内存的步骤一般是：

1.  **在核函数中声明共享内存数组：** 例如 `__shared__ float tile_A[TILE_SIZE][TILE_SIZE];`。大小根据问题而定，也可动态指定（kernel<<<..., sharedMemBytes>> 调用，核函数用 `extern __shared__` 接收）。
    
2.  **将全局内存数据加载到共享内存：** 通常每个线程加载一个或几个元素到共享内存数组适当位置。要注意边界条件（线程索引可能越界数据范围时不要读非法地址，可填 0 或判断）。
    
3.  **同步线程：**`__syncthreads()` 确保整个块都加载完成共享数据。
    
4.  **在共享内存上进行计算：** 此时数据已在快速内存中，可以在一个或多个循环中重复使用。比如矩阵乘法中，一个 16×16 的 tile 会被使用 16 次。
    
5.  **再次同步（如需要）：** 有些算法需要循环读取多个 tile 分块数据，多轮次需要同步确保前一轮计算用的数据不会被下一轮预取覆盖。
    
6.  **计算完成，将结果写回全局内存：** 输出结果往往直接写全局内存，如果数据量不大也可以暂存共享内存后一起写回。
    

以**矩阵乘法**的共享内存优化为例，核函数核心片段伪代码：

```cpp
__global__ void MatMulShared(float* A, float* B, float* C, int N) {
    __shared__ float As[TILE][TILE];
    __shared__ float Bs[TILE][TILE];
    int row = blockIdx.y * TILE + threadIdx.y;
    int col = blockIdx.x * TILE + threadIdx.x;
    float val = 0;
    for(int t = 0; t < N/TILE; ++t) {
        // 每个线程从全局内存将 A、B 一块数据读入共享内存
        if(row < N && t*TILE + threadIdx.x < N)
            As[threadIdx.y][threadIdx.x] = A[row * N + t*TILE + threadIdx.x];
        else
            As[threadIdx.y][threadIdx.x] = 0;
        if(col < N && t*TILE + threadIdx.y < N)
            Bs[threadIdx.y][threadIdx.x] = B[(t*TILE + threadIdx.y) * N + col];
        else
            Bs[threadIdx.y][threadIdx.x] = 0;
        __syncthreads();
        // 现在共享内存中有 TILE大小的子矩阵，进行局部乘加
        for(int i=0; i<TILE; ++i) {
            val += As[threadIdx.y][i] * Bs[i][threadIdx.x];
        }
        __syncthreads();
    }
    if(row < N && col < N)
        C[row * N + col] = val;
}
```

通过这样的方式，每个线程块处理 C 中的一个子方阵区域。共享内存的使用带来了两大好处：**减少全局内存访问**（每个矩阵元素在读取到共享内存后被 TILE 次使用，而不是每乘法一次读取），**提升访存连续性**（上例中 As 和 Bs 的加载都是按线程连续地址读取，符合合并访存要求）。这使得性能大幅提高。实际测试表明，用共享内存 tiling 的矩阵乘法相比天真逐元素乘法，性能可提高数十倍甚至上百倍（取决于矩阵大小和 GPU 型号）。

共享内存使用需要注意：

*   **容量限制：** 不同 GPU 每个 SM 的共享内存容量有限（比如大多数设备 48KB 或 96KB）。选择 TILE 大小时要算一下共享内存数组大小是否不超限制。
    
*   **线程分配：** 通常让线程块大小和 tile 大小匹配（如 16x16 线程处理 16x16 tile）。线程数也会影响计算能力利用和寄存器使用，要综合考虑。
    
*   **银行冲突：** 如 3.2 节末提到，共享内存由多个内存 bank 构成。简单来说，如果不同线程同时访问共享内存不同地址，但这些地址落在同一个内存 bank，就会发生冲突，使访问串行。解决方法包括：调整数据布局（比如采用结构化交错存储避免冲突）、或者在访问时避免线程访问模式过于有规律地落在同一 bank。上述矩阵乘法例子中索引设计已考虑避免冲突（典型方案是让连续线程访问共享内存不同列，从而落入不同 bank。大部分情况下，如果共享内存数组按 [threadIdx.y][threadIdx.x] 索引，就不太会有冲突，因为 threadIdx.x 相邻线程不同导致访问不同 bank。
    
*   **按需使用：** 并非所有核函数都需要共享内存优化。如果一次性访存很少重复，不需要复杂 tiling 反而增加同步开销。所以应把共享内存用在**重用率高**的场景。
    

### 3.4 合并访存与内存对齐

从上一节可以看出，**全局内存访问带宽**往往是 GPU 性能瓶颈之一。因此除了用共享内存，我们还应确保线程访问全局内存的模式尽量是**合并的**、对齐的，以减少实际内存事务数。

**合并访存（Coalesced Access）：** 当一个 warp（32 线程）访问全局内存时，如果这 32 个线程要访问的地址连续且对齐到 128 字节（因为 GPU 一个内存事务典型大小 128 字节）边界上，那么理想情况下，这个 warp 的所有访问可以通过**一个**内存事务完成。这种情况称为 100% 合并。举例来说，假设每个线程访问一个`float`（4 字节），那么一个 128 字节事务可包含 32 个 float。如果 thread0 访问地址 0， thread1 访问地址 4，... thread31 访问地址 124，这刚好连续 128 字节，这些访问会被硬件合并成一个请求发送到内存。反之，如果线程访问散乱，比如 thread0 访问 0, thread1 访问 1000，则无法合并，只能分别处理，开销增大。

因此，在设计数据结构和线程索引计算时，要**倾向于线性访问模式**。最典型的建议：**让 threadIdx.x 连续的线程访问连续的数组元素**。前面的矩阵加法和矩阵乘法例子都遵循这一点：如 `A[row * N + i*TILE + threadIdx.x]` 中，threadIdx.x 不同的线程访问地址相差 4 字节连续，方便 coalescing。

**对齐（Alignment）：** 内存对齐指数据结构在内存中的起始地址对某字节边界（如 16 字节、128 字节）对齐。对齐有助于合并和缓存效率。例如，如果一个结构体大小不是 2 的幂次，会造成跨界访问；或者数组不从 128 字节倍数开始（比如偏移 2 字节），warp 访问可能需要两个事务。CUDA API 提供 `cudaMallocPitch` 等帮助为二维数组分配对齐的 pitch 以优化行访问。通常标量数组自然对齐不用担心，但复杂数据或子数组要注意对齐起始地址。

**Stride 访存：** 指线程以固定步长间隔访问数据。如果步长较小且在 128 字节内，仍可合并，但如果步长很大，warp 线程可能访问完全不同区域，无法合并。例如，如果线程 0 访问 0, 线程 1 访问 256 字节处，32 线程分散在 32*256=8192 字节范围，肯定得多个事务。对策是尝试重排数据或算法，使访存尽可能连续。在必要情况，可借助共享内存进行 “访存模式变换”：比如将分散访问转换为合并加载到共享内存，再由线程各取所需。

**内存事务数观察：** 可以使用 Nsight Compute 采集指标如 `gld_efficiency`（全局内存加载效率）或 `dram_utilization` 等来量化访存效率。也可通过查看 `profiler` 日志看到合并比例。例如 GPU 有统计 “每个内存事务服务了多少字节” 等。

**案例：内存合并对性能的影响**  
考虑一个简单场景：有一个大型数组，我们有两个 kernel，一种 kernel 每个线程访问数组的连续元素（即线程 i 访问 A[i]），另一种 kernel 让线程 i 访问 A[ i*stride ]（stride 比如 2 或更高）。第二种情况 warp 内线程访问间隔较大，会明显观察到内存吞吐下降，kernel 执行变慢。这样的实验可以让读者直观体会合并访存的重要性。

总而言之，**高效的 CUDA 程序应确保全局内存访存模式友好**：邻近线程访问邻近地址，尽量对齐边界，减少不必要的散乱访问。如有不规则访问，可以考虑引入额外步骤（排序、分块、使用共享内存暂存）来优化。

### 3.5 其他优化建议

除了上述主要优化方向，还有一些常见的 CUDA 优化原则简要提及：

*   **提高线程占用（occupancy）：** 即让每个 SM 上运行足够多的 warp 以隐藏延迟。占用率由线程块尺寸、寄存器和共享内存使用决定。一般 256 或 512 线程的块是常用配置，可较好占用 SM。如果块太小，会有很多空闲资源；块太大可能导致寄存器不够反而降低并发。Nsight Compute 能报告 kernel 的 occupancy，可参考优化。
    
*   **减少寄存器溢出：** 每线程可用寄存器有限，若编译器报本地内存使用（寄存器溢出到本地内存）会拖慢速度。可尝试降低代码复杂度、减少大型局部数组或使用 `-maxrregcount` 编译选项调节寄存器使用上限。
    
*   **循环展开和内联：** 对小循环，编译器通常会展开从而减少循环控制开销。可以手工使用 `#pragma unroll` 建议编译器展开循环。函数调用可使用 `__device__ __forceinline__` 提示内联，减少函数调用开销（GPU 对深层小函数调用支持不如 CPU 强）。
    
*   **使用快速数学函数：** CUDA 有些设备函数有快速版本如 `__sinf()` 比 `sinf()` 更快但稍低精度，可在对精度不敏感处使用。
    
*   **并发执行和流水：** 高级一点的，如利用 Streams 让数据传输和计算重叠，第四章详述。
    

当然，优化应以正确性为前提。每次优化改动都应测试结果是否仍正确，再比较性能提升。常见的做法是使用较小数据跑一遍 CPU 单线程版本和 GPU 版本结果对比验证正确，然后对比性能。

### 3.6 错误检查与调试技巧

在优化过程中，我们也要学会捕捉和修复 CUDA 程序中的错误（bugs）。GPU 编程的错误有时比较隐蔽，例如：

*   越界访问显存（不会像 CPU 那样崩溃但结果错误或不稳定）。
    
*   忘记同步导致数据竞争或读未更新数据。
    
*   没有检查 API 返回值导致错误被静默忽略。
    

**CUDA 错误检查宏：** 建议写一个宏包装 CUDA 调用，如：

```cpp
#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line) {
   if (code != cudaSuccess) {
      fprintf(stderr,"GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
      exit(code);
   }
}
```

然后将 `cudaMalloc(&ptr, size)` 替换为 `gpuErrchk(cudaMalloc(&ptr, size));`，这样一旦发生错误会立刻打印错误信息和发生位置。尤其是在调试阶段，加上这些检查有助于发现例如内存分配失败、非法地址等问题。

**同步和 lastError：** kernel 调用是异步的，如果立刻调用 cudaGetLastError() 可以获取上一次 kernel launch 是否有同步检测到的错误。例如：

```cpp
kernel<<<grid,block>>>(...);
cudaError_t err = cudaGetLastError();
if(err != cudaSuccess) { printf("Kernel Launch Error: %s\n", cudaGetErrorString(err)); }
```

另外在要确定 kernel 真的完成后，可用 `cudaDeviceSynchronize()` 再检查 `cudaGetLastError()` 以捕获运行时错误（如越界导致的 memory access error）。

**cuda-memcheck 工具：** NVIDIA 提供类似 Valgrind 的内存检查工具 `cuda-memcheck`，可以检测常见的错误：

```sh
cuda-memcheck ./my_program
```

它会报告诸如非法内存访问、越界、未初始化内存使用等。开发中强烈建议使用它来验证内核不存在明显错误（尤其是越界访问，这在 CPU 上可能不会报错但 GPU 上会导致错误结果）。

**printf 调试：** CUDA 内核中可使用 `printf`（需 GPU 架构≥2.0），在 device 上输出调试信息，这在定位逻辑问题时很有帮助。但注意过多 printf 会影响性能和可能淹没输出，所以只在小规模数据或定位问题时使用。此外，printf 输出在 kernel 结束后才统一打印，要注意查看顺序可能不按线程顺序。

**调试模式编译：** 使用 `-G` 编译选项可以生成 GPU 代码的调试信息，允许在 Nsight 或 cuda-gdb 中逐步调试。`-G`会使编译的代码未做很多优化，执行会慢，且寄存器使用变高，一些错误（如线程间影响）可能会因优化关闭而暂时消失。所以调试版主要用来逻辑跟踪，性能测试还是用 release 版编译（无 - G）。

**cuda-gdb:** Linux 下可以使用 `cuda-gdb` 命令行调试 CUDA 程序。可以像 gdb 那样设断点、单步。不过调试 GPU 常常需要指定要调试哪个 thread。例如在 cuda-gdb 中可以用 `thread apply all bt` 查看所有线程的 backtrace 等，也可以限定某个线程。Nsight VS Edition 在 Windows 上提供了更直观的界面，可以观察变量、线程格局等，更易用。

**常见错误及提示：**

*   **kernel launch failed:** 若 `cudaGetLastError()` 报 launch error，可能配置 (block/thread 数) 不正确（如每块线程超过限制，或总线程数 0），也可能是前一次 kernel 有错误未处理。
    
*   **无输出或结果错误:** 检查是否忘了 `cudaMemcpy` 回主机，或在 kernel 未完成就用了数据（需同步）。
    
*   **错位结果:** 可能是计算 global index 错了，使一部分元素没写或者重复写。此时打印 threadIdx/blockIdx 以及处理的索引可以帮助找出。
    
*   **性能不佳:** 可用 Nsight Compute 查看 memory throughput，是否出现很多 memory throttle、divergence 等，从而对症下药（如未合并访存、bank 冲突等）。
    

通过良好的错误检查和调试手段，我们可以更快定位 CUDA 程序的问题，为优化打下基础。虽然 GPU 编程调试相对复杂，但多利用工具和经验规则，完全可以将问题各个击破。

#### 本章小结

*   编写核函数时，要合理组织线程索引计算，使每个线程处理唯一数据元素并覆盖全部数据。使用 `__global__` 定义核函数，并通过 `<<<>>>` 指定执行配置启动之。
    
*   为提高性能，应避免 **warp 分支发散**。可通过调整算法逻辑、使用条件运算符、warp 内通信等方法降低发散。尽量使 warp 内线程执行统一路径。
    
*   **共享内存**是优化关键手段，可用于数据重用和缓存。采用 tiling 技术将全局内存的数据块载入共享内存供线程块内部重复使用，可极大减少全局访存次数。使用共享内存需注意容量和访问模式避免 bank 冲突。
    
*   **合并访存和对齐**能提高内存带宽利用率。同一 warp 的线程应尽量访问连续内存，从而合并成少量内存事务。保证数据对齐以避免跨越多个事务。非连续访问的情况应设法优化或重排数据。
    
*   开发优化过程中，利用 **cudaGetLastError**、自定义宏、`cuda-memcheck` 等工具检测错误。调试 GPU 代码可使用 Nsight 或 cuda-gdb，配合 `-G` 编译选项和内核中的 printf 调试定位问题。确保每次优化没有引入错误，再比较性能收益。
    

#### 练习题

1.  编写一个 CUDA 核函数来计算数组元素的平方和（即 sum = Σ x[i]^2）。让每个线程计算一部分，然后使用 **共享内存** 在块内进行局部归约，最后由块内线程 0 输出部分和到全局内存。注意块内同步，并考虑如何避免不同块之间的数据竞争。
    
2.  将第 1 题的核函数改进，尝试避免 warp 分支发散。例如在归约时使用循环而非 if 去判断奇偶线程。另外，可以使用 `__shfl_down_sync`（若熟悉）进一步优化归约（可选挑战）。
    
3.  针对矩阵乘法，分别实现**不使用共享内存**的简单版本和**使用共享内存 tiling** 的版本。对比两者在不同矩阵维度（比如 512、1024）的运行时间。观察共享内存带来的加速比随着矩阵尺寸变化的情况。
    
4.  设计一个实验验证合并访存的重要性：创建两个核函数，一个让线程连续访问全局数组元素，另一个让线程访问跳跃的元素（如每个线程访问两段相距较远的数据）。使用 Nsight Compute 或计时比较两者带宽利用。尝试通过调整跳跃步长，找到性能明显下降的临界步长。
    
