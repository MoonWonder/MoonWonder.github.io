---
title: CUDA 基础知识
subtitle:
date: 2025-11-17T17:34:57+08:00
description:
keywords:
draft: false
---
> 本章将介绍 CUDA 并行编程模型的核心概念，包括线程层次结构（线程、线程块、网格）、CUDA 的内存模型（寄存器、共享内存、全局内存等）以及基本的异构编程思想。理解这些基础知识将帮助我们在后续章节中编写高效的 CUDA 程序。

### 1.1 线程、块与网格

CUDA 使用一种层次化的并行模型，允许开发者以**线程（Thread）** 为基本单位来表达并行计算。一个在 GPU 上运行的 CUDA 函数被称为**核函数（Kernel）**，当我们在主机端（CPU）调用核函数时，实际上会**启动众多并行的线程在 GPU 上执行**该函数，每个线程处理数据的一部分 。为了组织和管理大量线程，CUDA 引入了 **线程块（Block）** 和 **网格（Grid）** 的概念。

*   **线程（Thread）：** CUDA 中执行核函数的基本并行单元。每个线程都有一个唯一的线程索引，可以通过内建变量如 `threadIdx` 在核函数中获取。线程负责处理数据的基本元素，例如处理数组中的一个元素或矩阵中的一个单元。
    
*   **线程块（Block）：** 线程按照一定的布局被组织成线程块。一个线程块可以包含最多 1024 个线程（硬件限制），线程块内部的线程可以通过高速共享内存互相协作，并可以使用线程同步原语（如`__syncthreads()`）等待彼此。线程块有一维、二维或三维的组织形式（由 `blockDim` 定义其各维大小），方便映射多维数据（比如图像的二维像素网格）。
    
*   **网格（Grid）：** 多个线程块组成网格。一次核函数启动对应一个网格，网格可以是一维、二维或三维的，由 `gridDim` 定义每个维度上线程块的数量。所有线程块一同并发执行同一个核函数，但彼此之间相对独立：不同块的线程**无法直接通信**，只能通过全局内存间接交换数据。
    

 _图 1：CUDA 线程层次结构示意图。核函数启动时，在 GPU 上生成一个由多个线程块组成的网格，每个线程块包含若干并行线程。线程块内的线程可共享数据和同步，但不同线程块之间相互独立。_

当我们在主机代码中使用 `<<<gridDim, blockDim>>>` 语法启动 CUDA 核函数时，需要指定网格中线程块数量 (`gridDim`) 和每个块中的线程数量 (`blockDim`)。例如： `kernel<<<numBlocks, threadsPerBlock>>>(...)`。CUDA 内置了三维索引变量 `blockIdx` 和 `threadIdx`，使得每个线程可以通过组合**块索引**和**线程索引**来计算出全局唯一的线程 ID，从而确定其需要处理的数据元素。典型情况下，可以这样计算全局索引：

```cpp
int global_x = blockIdx.x * blockDim.x + threadIdx.x;
int global_y = blockIdx.y * blockDim.y + threadIdx.y;
int globalIndex = global_y * Width + global_x; // 例如用于二维数据
```

上述计算中，`blockDim`表示每个块的尺寸，`blockIdx`表示当前线程所在块的坐标，`threadIdx`是线程在块内的坐标。通过这些，我们可以让每个线程处理全局数据数组的不同元素。例如，对于二维网格和二维块组织的情况，可让 `(blockIdx.x, blockIdx.y)` 确定线程块处理矩阵的哪一部分，而 `(threadIdx.x, threadIdx.y)` 确定线程在该块内处理子块的哪一个元素。

需要注意的是，CUDA 架构对每个线程块的线程数有上限（目前通常是 1024）。同时，GPU 硬件以 **流式多处理器（SM）** 为单位调度线程块——每个线程块在硬件上会被调度到一个 SM 上执行。在执行过程中，一个 SM 可以同时驻留执行多个线程块（取决于块所需资源和 SM 资源容量）。**一个线程块不会迁移到其它 SM 上执行**（除非发生特殊情况如预警性调度或调试），这意味着块内线程始终在同一 SM 上，因此可以高效地共享数据。这种调度模型也解释了为何块内线程可以快速同步，而不同块之间无法直接同步（它们可能运行在不同的 SM 上）。

综上，CUDA 的线程组织是二维的：在一个核函数中有若干线程块 (Block)，组成一个网格 (Grid)。每个线程块内又包含多个线程 (Thread)。这种层次结构有助于我们将问题分解为小块、再由线程并行解决，并且能适应 GPU 硬件的分配策略 。在实际编程中，开发者需要根据问题规模和数据结构选择合适的块大小和网格维度，以充分利用 GPU 资源。一般经验是让网格总线程数≥GPU 核心数的数倍，以隐藏延迟、提高吞吐，但同时**每个线程块的大小也不宜过小或过大**，需要综合考虑共享内存使用、寄存器限制和线程调度效率。

### 1.2 SIMD 和 Warp（线程束）

理解 CUDA 并行执行模型，还需要提及 GPU 硬件的执行粒度：**Warp（线程束）**。在 GPU 硬件中，线程并不是一个一个独立调度执行的，而是以 “**Warp**” 为单位一起执行。一个 Warp 通常由 32 个线程组成（NVIDIA GPU 长期以来定义 warp 大小为 32）。**Warp 内的 32 个线程会同步执行相同的指令**（Single Instruction Multiple Data，SIMD）——这意味着在没有分支分歧的情况下，一个 warp 的线程几乎是锁步执行的。warp 中各线程可以处理不同的数据元素，但如果程序执行遇到分支条件，不同线程可能走向不同分支，从而导致 **Warp 分支发散** 的情况。

当 warp 中线程发生了条件分支（例如 `if/else`），且不同线程取不同分支路径时，GPU 硬件将**顺序执行各个分支**，执行一条分支时让不属于该分支的线程处于等待 / 空转状态，从而保证正确性。这种现象称为 **Warp Divergence（分支发散）**。分支发散会降低并行效率，因为原本可以并行执行的 warp 被迫串行化执行多个路径，导致部分硬件资源闲置。“Warp 分支发散” 是 CUDA 优化需尽量避免的一个问题。举例来说，有下面的核函数：

```cpp
__global__ void compute(int *data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(idx % 2 == 0) {
        data[idx] += 3;
    } else {
        data[idx] = idx;
    }
}
```

对于 warp 大小 32 的 GPU，当 `idx` 有奇有偶时，一个 warp 中的线程将不统一：一部分执行 if 分支，另一部分执行 else 分支。这时 GPU 将先执行 if 分支语句（让属于 else 的线程暂停），再执行 else 分支语句。这样一个 warp 用了两步完成本应一步完成的工作，效率降低。**避免分支发散的思路**是让同一个 warp 内的线程尽量执行相同的指令路径。例如可以通过数学运算替换条件赋值，或使用 CUDA 提供的谓词指令等手段，实现**不分支**却达到同样效果。当然，有时分支不可避免，此时我们也无需为少量不可避免的发散而过度修改代码，只需保证多数线程执行一致路径即可。

总之，了解 warp 的存在很重要：**GPU 在线程级并不是完全独立并行的，而是在 warp 粒度上 SIMD 执行**。这提醒我们在编写核函数时应注意：尽量让 warp 内线程遵循相似的控制流，避免严重的执行路径分歧，以充分利用 GPU 并行执行效率。

### 1.3 CUDA 内存层次结构

GPU 不仅在计算模型上有其特殊性，在内存架构上也与 CPU 有显著区别。**CUDA 内存层次结构**体现在多个不同速度和作用域的存储区域，合理地使用这些内存对于实现高性能十分关键。

CUDA 可编程的主要内存类型包括：

*   **寄存器（Register）：** 每个线程私有的高速存储，由硬件自动分配。寄存器是最快的存储空间，但数量有限。用于存放线程的局部变量（标量、部分数组元素等）。线程无法直接访问彼此的寄存器。
    
*   **本地内存（Local Memory）：** 如果线程需要的局部数据超过寄存器容量，或是大数组等会被编译器放入 “本地内存”。尽管叫本地内存，其实它位于设备全局内存中，通过缓存访问，速度相对较慢。
    
*   **共享内存（Shared Memory）：** 也称片上内存，每个线程块内部的线程都可以访问的**高速存储区域**。共享内存驻留在每个 SM 上，延迟非常低（接近寄存器），典型大小每个块可用 48KB（不同 GPU 架构可能为 48KB 或 96KB，可配置）。线程块内共享内存实现了线程间快速协作：一个线程可以将数据写入共享内存，块内其他线程可几乎即时读到。这对于实现块内数据重用、减少全局内存访问非常有用。共享内存的生命周期限定在核函数调用期间，块结束时释放。注意，共享内存分为若干**内存银行**，访问模式不当会引起**银行冲突**（多线程同时访问同一内存银行会串行化），这一点在后续优化章节详述。
    
*   **全局内存（Global Memory）：** GPU 的显存（Device Memory）空间，又称设备全局内存。容量相对大（几 GB 到数十 GB 不等），但访问延迟也高。所有线程（不论哪个块）都可读写全局内存。我们通过 `cudaMalloc` 获得的 GPU 内存、以及内核函数参数数组，都位于全局内存。由于较高的访问延迟，GPU 还配备了缓存（如每个 SM 有 L1 / 共享内存统一缓存，以及所有 SM 共享的 L2 缓存）来加速全局内存访问。开发者需要注意**合并访存**（coalesced memory access）：当同一 warp 的线程访问全局内存中连续地址时，硬件会将这些访问合并为更少的内存事务，从而更高效地利用内存带宽。如果访存不连续或不对齐，则可能拆分为多个事务，效率降低。
    
*   **常量内存（Constant Memory）和纹理内存（Texture Memory）：** 这两者也是位于全局内存但有特殊用途的只读内存区域。常量内存用于存放少量在设备上只读的参数，由硬件提供广播缓存机制，一个 warp 中所有线程若访问同一常量地址，只产生一次内存读取。纹理内存则针对图像 / 数组提供了缓存和特殊的读取机制（如地址绑定、二维缓存局部性等）。这些高级特性在本书不做深入展开，但了解它们存在即可。
    

_图 2：GPU 硬件的内存层次结构示意图（以 NVIDIA A100 为例）。SM（流式多处理器）内部包含快速但容量小的寄存器文件和共享内存 / L1 缓存，每个线程块运行于单个 SM 上，共享该 SM 的共享内存。多个 SM 共同连接到更大的 L2 缓存和全局显存。这种分层结构兼顾了速度和容量。_

上述内存空间各有不同的作用域和性能特点。一般来说，**寄存器**最快但每线程私有；**共享内存**很快且块内共享；**全局内存**容量大但慢（需通过缓存或手动优化访问模式提高有效带宽）。高效的 CUDA 程序会尽量**重用数据**：例如，把需要反复使用的数据从全局内存加载到共享内存，再由多个线程重复使用，从而减少全局内存访问次数。在后续章节，我们会看到利用共享内存优化矩阵乘法、利用缓存和合并访存优化内存带宽的方法。总之，理解内存层次有助于编写高性能代码：**将数据放到适当的位置，并以高效模式访问**。

### 1.4 GPU 与 CPU 的异构执行

CUDA 编程属于**异构编程**：程序同时使用 CPU（主机）和 GPU（设备）协同工作。一般流程是：**在 CPU 端准备数据，将数据拷贝到 GPU，在 GPU 上并行执行核函数，然后将结果拷回 CPU**。主机代码用普通 C++ 编写，设备端代码（核函数）使用 CUDA C/C++ 扩展编写。当我们调用 `kernel<<<...>>>(...)` 启动核函数时，实际上发生了以下步骤：

1.  **数据传输**：需要处理的输入数据从主机内存复制到设备全局内存（又称 Host-to-Device 拷贝）。
    
2.  **核函数执行**：GPU 接管执行，启动指定数量的线程并行运行核函数代码。执行期间 GPU 可以访问自身全局内存中的数据，并使用寄存器、共享内存等加速计算。在执行过程中，主机 CPU 通常是异步返回的（kernel launch 是异步调用），CPU 可以做其他事情或继续发起其它 GPU 工作。
    
3.  **结果传输**：核函数执行完成后，将结果数据从设备内存复制回主机内存（Device-to-Host 拷贝），供 CPU 继续处理或输出。
    

需要注意**异步执行模型**：CUDA 中 GPU 的核函数调用默认是异步的，主线程不会等待 GPU 计算完成就继续往下执行。为了确保结果已经计算完毕再使用，我们通常在拷贝回数据前，或通过显示同步调用（如 `cudaDeviceSynchronize()`）等待 GPU 完成。这种默认异步的好处是可以让 CPU 和 GPU **重叠工作**：例如 CPU 可以在 GPU 计算时准备下一批数据。后续第四章会深入讨论 CUDA 的异步执行机制和流的概念。

总之，本章介绍了 CUDA 基础的编程模型概念：线程 / 块 / 网格划分并行任务，warp 级执行以及分支发散问题，GPU 内多级存储和数据传输的特点。这些构成理解后续章节的基石。您现在应明白，每当您编写并启动一个 CUDA 核函数时，实际上是在 GPU 上启动了大量线程按照您定义的网格结构并行运行，而高性能的秘诀在于合理安排这些线程的工作及数据访问模式，使 GPU 硬件资源得到充分利用。

#### 本章小结

*   CUDA 将并行任务抽象为**核函数**在 GPU 上执行，大量线程并行运行同一核函数实例，每个线程处理整体任务的一部分。
    
*   **线程**是基本执行单元，**线程块 Block** 是线程的分组，共享一定资源（共享内存等），**网格 Grid** 包含多个线程块组成完成整个并行任务。
    
*   GPU 硬件按 **warp（线程束）** 调度执行，一般每 32 个线程为一组同步执行。如果 warp 内出现分支，需串行化执行各分支路径，造成性能损失（分支发散），应尽量避免。
    
*   CUDA 内存分层：线程有私有**寄存器**和**本地内存**，块内线程共享**共享内存**，所有线程可访问**全局内存**。应善用共享内存缓存重复数据，优化全局内存访存模式（合并访存）以提高带宽利用。
    
*   CUDA 程序为异构执行：CPU 管理逻辑和数据传输，GPU 加速并行计算。需了解数据在主机和设备间传输的成本，以及 CUDA 的异步执行特点，合理安排 CPU-GPU 协作。
    

#### 练习题

1.  回顾本章内容，用自己的话解释什么是线程块和网格，以及它们为何设计成三维结构？
    
2.  为什么 warp 分支发散会降低 GPU 执行效率？请举一个简单的代码示例说明如何避免或减轻分支发散。
    
3.  列举 CUDA 内存层次的各个类型，并说明它们的速度和作用域差异。想一想，在矩阵乘法这样需要多次重复访问数据的核函数中，如何利用共享内存提升性能？
    
4.  编写一个 CUDA 核函数，实现两个数组的逐元素相加（C[i] = A[i] + B[i]）。确定一个合理的网格配置（线程块大小和块数），并尝试计算当数组长度为任意值时，该配置如何覆盖所有元素。在主机端调用核函数并验证结果正确性。
